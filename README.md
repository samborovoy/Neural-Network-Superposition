# Neural-Network-Superposition
This repo explores how neural networks represent features in activation space, focusing on MLP blocks in Transformers. It examines superposition, enabling networks to encode more features than neurons, and how input sparsity drives polysemanticity, where neurons encode multiple features, reducing interpretability.
